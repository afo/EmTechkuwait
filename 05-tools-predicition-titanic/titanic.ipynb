{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World: AI, Machine Learning & Data Science \n",
    "\n",
    "---\n",
    "\n",
    "## Why Python?\n",
    "\n",
    "Python is a general programming language that can do both Software development, Web development, sophisticated mathematical / statistical modeling. Python has experienced incredible growth over the last couple of years, and many of the state of the art Machine Learning libraries being developed today have support for Python.\n",
    "\n",
    "![](https://zgab33vy595fw5zq-zippykid.netdna-ssl.com/wp-content/uploads/2017/09/growth_major_languages-1-1400x1200.png)\n",
    "\n",
    "Source: https://stackoverflow.blog/2017/09/06/incredible-growth-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is free!\n",
    "\n",
    "Most of the best software in the world today is open source, even enterprise-ready software. That means that most packages, modules, libraries, and implementations are free. Anyone can download them and use them in any part of the world.\n",
    "\n",
    "**Examples of great, free AI libraries**\n",
    "* Google's TensorFlow\n",
    "* Scikit-learn\n",
    "* Pandas\n",
    "* Keras\n",
    "* Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations of all State-of-the-Art algorithms\n",
    "\n",
    "No matter if you want to use Linear Regression, Random Forests, Neural Networks, or Deep Learning all of the latest methods are implemented optimized for Python.\n",
    "\n",
    "## Big Data Compatible\n",
    "\n",
    "Code, programs, and software created with Python can be run on any computer. Therefore, you can scale your computations and utilize for example cloud resources to run big data jobs.\n",
    "\n",
    "**Great tools for big data computations:**\n",
    "- Spark\n",
    "- Databricks\n",
    "- Amazon EC2\n",
    "- Amazon S3\n",
    "- MLFlow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real world example of AI: Titanic Analysis\n",
    "\n",
    "Titanic notebook! Open Source: Anyone can do this! We host all our teaching materials online. Democratize it. Startups can even develop the most sophisticated AI solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The difficult part is never to implement the algorithm\n",
    "\n",
    "The hard part of a machine learning problem is to get the correct data that can solve your problem. We'll illustrate this with an example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# __Titanic Survivor Analysis__\n",
    "\n",
    "\n",
    "**Sources:** \n",
    "* **Training + explanations**: https://www.kaggle.com/c/titanic\n",
    "\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding the connections between passanger information and survival rate\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    "**Our task is to train a machine learning model on the training set in order to predict if the passengers in the test set survived or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussian Naive Bays\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from plot_distribution import plot_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "___\n",
    "## Part 2: Exploring and Preprocessing the Data\n",
    "**Data descriptions**\n",
    "\n",
    "<img src=\"data/Titanic_Variable.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General data statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(13,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set?\n",
    "df['Survived'].map({0:'Died',1:'Survived'}).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "> #### __Brief Remarks Regarding the Data__\n",
    "\n",
    "> * `PassengerId` is a random number (incrementing index) and thus does not contain any valuable information. \n",
    "\n",
    "> * `Survived, Passenger Class, Age, Siblings Spouses, Parents Children` and `Fare` are numerical values (no need to transform them) -- but, we might want to group them (i.e. create categorical variables). \n",
    "\n",
    "> * `Sex, Embarked` are categorical features that we need to map to integer values. `Name, Ticket` and `Cabin` might also contain valuable information.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unnecessary data\n",
    "__Note:__ It is important to remove variables that convey information already captured by some other variable. Doing so removes the correlation, while also diminishing potential overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'Ticket', 'Cabin', need to do it for both test and training\n",
    "\n",
    "df = df.drop(['PassengerId','Ticket', 'Cabin','Fare'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "____\n",
    "## Part 3: Establishing Hypotheses\n",
    "\n",
    "### _The Title of the person is a feature that can predict survival_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List example titles in Name column\n",
    "df.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column called Title\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that our titles makes sense (by comparing to sex)\n",
    "pd.crosstab(df['Title'], df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Title'] = df['Title'].\\\n",
    "              replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr',\\\n",
    "             'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "df['Title'] = df['Title'].replace('Mlle', 'Miss') #Mademoiselle\n",
    "df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Mme', 'Mrs') #Madame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have more logical (contemporary) titles, and fewer groups\n",
    "\n",
    "df[['Title', 'Survived']].groupby(['Title']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can plot the survival chance for each title\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Title\", data=df, order=[1,0])\n",
    "plt.xticks(range(2),['Survived','Deceased']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title dummy mapping: Map titles to binary dummy columns\n",
    "\n",
    "binary_encoded = pd.get_dummies(df.Title)\n",
    "df[binary_encoded.columns] = binary_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unique variables for analysis (Title is generally bound to Name, so it's also dropped)\n",
    "df = df.drop(['Name', 'Title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Sex column to binary (male = 0, female = 1) categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variable to numeric\n",
    "\n",
    "df['Sex'] = df['Sex']. \\\n",
    "    map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values for age\n",
    "We will now guess values of age based on sex (male / female) \n",
    "and socioeconomic class (1st, 2nd, 3rd) of the passenger.\n",
    "\n",
    "The row indicates the sex, male = 0, female = 1\n",
    "\n",
    "> __IDEA:__ Wealth (indicated by class accomodation), as well as Gender, historically are indicative of age. \n",
    "\n",
    "> This approach gives us a more refined estimate than only taking the median / mean, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty array  for later use\n",
    "\n",
    "guess_ages = np.zeros((2,3),dtype=int) \n",
    "guess_ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NA's for the Age columns\n",
    "# with \"qualified guesses\"\n",
    "\n",
    "print('Guess values of age based on sex and pclass of the passenger')\n",
    "for i in range(0, 2):\n",
    "    for j in range(0,3):\n",
    "        guess_df = df[(df['Sex'] == i) \\\n",
    "                    &(df['Pclass'] == j+1)]['Age'].dropna()\n",
    "\n",
    "        # Extract the median age for this group\n",
    "        # (less sensitive) to outliers\n",
    "        age_guess = guess_df.median()\n",
    "\n",
    "        # Convert random age float to int\n",
    "        guess_ages[i,j] = int(age_guess)\n",
    "\n",
    "\n",
    "print('Guess_Age table:\\n',guess_ages)\n",
    "\n",
    "for i in range(0, 2):\n",
    "    for j in range(0, 3):\n",
    "        df.loc[ (df.Age.isnull()) & (df.Sex == i) \\\n",
    "                & (df.Pclass == j+1),'Age'] = guess_ages[i,j]\n",
    "\n",
    "\n",
    "df['Age'] = df['Age'].astype(int)\n",
    "print()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split age into bands / categorical ranges and look at survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age bands\n",
    "df['AgeBand'] = pd.cut(df['Age'], 5)\n",
    "df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False)\\\n",
    "                    .mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of suvival relative to age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived \n",
    "# or did not survive\n",
    "\n",
    "plot_distribution( df , var = 'Age' , target = 'Survived' ,\\\n",
    "                  row = 'Sex' )\n",
    "\n",
    "# Recall: {'male': 0, 'female': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Age column to\n",
    "# map Age ranges (AgeBands) to integer values of categorical type \n",
    "\n",
    "\n",
    "df.loc[ df['Age'] <= 16, 'Age'] = 0\n",
    "df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\n",
    "df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2\n",
    "df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3\n",
    "df.loc[ df['Age'] > 64, 'Age']=4\n",
    "df = df.drop(['AgeBand'], axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Note we could just run \n",
    "# df['Age'] = pd.cut(df['Age'], 5,labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Party Size\n",
    "\n",
    "How did the number of people the person traveled with impact the chance of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp = Number of Sibling / Spouses\n",
    "# Parch = Parents / Children\n",
    "\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# Survival chance against FamilySize\n",
    "df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True) \\\n",
    "                                .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it, 1 is survived\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"FamilySize\", data=df, order=[1,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable if the person was alone or not\n",
    "\n",
    "df['IsAlone'] = 0\n",
    "df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use the binary IsAlone feature for further analysis\n",
    "\n",
    "df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create new features based on intuitive combinations\n",
    "# Here is an example when we say that the age times socioclass is a determinant factor\n",
    "\n",
    "df['Age*Class'] = df.Age.values * df.Pclass.values\n",
    "\n",
    "df.loc[:, ['Age*Class', 'Age', 'Pclass']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Port the person embarked from\n",
    "Let's see how that influences chance of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"data/images/titanic_voyage_map.png\">\n",
    ">___\n",
    "\n",
    "> #### __Interesting Fact:__ \n",
    "\n",
    "> Third Class passengers were the first to board, with First and Second Class passengers following up to an hour before departure. \n",
    "\n",
    "> Third Class passengers were inspected for ailments and physical impairments that might lead to their being refused entry to the United States, while First Class passengers were personally greeted by Captain Smith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN 'Embarked' Values in the dfs\n",
    "freq_port = df['Embarked'].dropna().mode()[0]\n",
    "\n",
    "df['Embarked'] = df['Embarked'].fillna(freq_port)\n",
    "    \n",
    "df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True) \\\n",
    "                    .mean().sort_values(by='Survived', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical dummy variables for Embarked values\n",
    "\n",
    "binary_encoded = pd.get_dummies(df.Embarked)\n",
    "df[binary_encoded.columns] = binary_encoded\n",
    "df.drop('Embarked', axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished -- Preprocessing Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features are approximately on the same scale\n",
    "# no need for feature engineering / normalization\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: View the correlation between features in our processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrelated features are generally more powerful predictors\n",
    "\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(df.corr().round(2)\\\n",
    "            ,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \\\n",
    "            linecolor='white', annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "___\n",
    "### Machine Learning, Prediction and Artifical Intelligence\n",
    "Now we will use Machine Learning algorithms in order to predict if the person survived. \n",
    "\n",
    "**We will choose the best model from:**\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN) \n",
    "3. Support Vector Machines (SVM)\n",
    "4. Perceptron\n",
    "5. XGBoost\n",
    "6. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Survived\", axis=1) # Training & Validation data\n",
    "Y = df[\"Survived\"]              # Response / Target Variable\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set so that we validate on 20% of the data\n",
    "# Note that our algorithms will never have seen the validation \n",
    "# data during training. This is to evaluate how good our estimators are.\n",
    "\n",
    "np.random.seed(1337) # set random seed for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = \\\n",
    "                train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print('Training Samples:', X_train.shape, Y_train.shape)\n",
    "print('Validation Samples:', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "> ## General ML workflow\n",
    "> 1. Create Model Object\n",
    "> 2. Train the Model\n",
    "> 3. Predict on _unseen_ data\n",
    "> 4. Evaluate accuracy.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Prediciton Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()           # create\n",
    "logreg.fit(X_train, Y_train)            # train\n",
    "acc_log_2 = logreg.score(X_val, Y_val)  # predict & evaluate\n",
    "\n",
    "print('Logistic Regression accuracy:',\\\n",
    "      str(round(acc_log_2*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)                  # instantiate\n",
    "knn.fit(X_train, Y_train)                                    # fit\n",
    "acc_knn = knn.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('K-Nearest Neighbors labeling accuracy:', str(round(acc_knn*100,2)),'%')                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines Classifier (non-linear kernel)\n",
    "svc = SVC()                                                  # instantiate\n",
    "svc.fit(X_train, Y_train)                                    # fit\n",
    "acc_svc = svc.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('Support Vector Machines labeling accuracy:', str(round(acc_svc*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()                                    # instantiate \n",
    "perceptron.fit(X_train, Y_train)                             # fit\n",
    "acc_perceptron = perceptron.score(X_val, Y_val)              # predict + evalaute\n",
    "\n",
    "print('Perceptron labeling accuracy:', str(round(acc_perceptron*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost, same API as scikit-learn\n",
    "gradboost = xgb.XGBClassifier(n_estimators=1000)             # instantiate\n",
    "gradboost.fit(X_train, Y_train)                              # fit\n",
    "acc_xgboost = gradboost.score(X_val, Y_val)                  # predict + evalute\n",
    "\n",
    "print('XGBoost labeling accuracy:', str(round(acc_xgboost*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=500)   # instantiate\n",
    "random_forest.fit(X_train, Y_train)                         # fit\n",
    "acc_rf = random_forest.score(X_val, Y_val)                  # predict + evaluate\n",
    "\n",
    "print('Random Forest labeling accuracy:', str(round(acc_rf*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Neural Networks (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Dense(units=300, activation='relu', input_shape=(13,) ))\n",
    "model.add( Dense(units=100, activation='relu'))\n",
    "model.add( Dense(units=50, activation='relu'))\n",
    "model.add( Dense(units=1, activation='sigmoid') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs = 100, batch_size= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model Accuracy on test set\n",
    "print('Neural Network accuracy:',str(round(model.evaluate(X_val, Y_val, batch_size=50,verbose=False)[1]*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance scores in the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at importnace of features for random forest\n",
    "\n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print ('Training accuracy Random Forest:',model.score( X , y ))\n",
    "\n",
    "plot_model_var_imp(random_forest, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "___\n",
    "\n",
    "## Appendix I:\n",
    "#### Why are our models maxing out at around 80%?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __John Jacob Astor__\n",
    "\n",
    "<img src= \"data/images/john-jacob-astor.jpg\"> \n",
    "\n",
    "John Jacob Astor perished in the disaster even though our model predicted he would survive. Astor was the wealthiest person on the Titanic -- his ticket fare was valued at over 35,000 USD in 2016 -- it seems likely that he would have been among of the approximatelly 35 percent of men in first class to survive. However, this was not the case: although his pregnant wife survived, John Jacob Astor’s body was recovered a week later, along with a gold watch, a diamond ring with three stones, and no less than 92,481 USD (2016 value) in cash.\n",
    "\n",
    "<br >\n",
    "\n",
    "\n",
    "#### __Olaus Jorgensen Abelseth__\n",
    "\n",
    "<img src= \"data/images/olaus-jorgensen-abelseth.jpg\">\n",
    "\n",
    "Avelseth was a 25-year-old Norwegian sailor, a man in 3rd class, and not expected to survive by classifier. However, once the ship sank, he survived by swimming for 20 minutes in the frigid North Atlantic water before joining other survivors on a waterlogged collapsible boat.\n",
    "\n",
    "Abelseth got married three years later, settled down as a farmer in North Dakota, had 4 kids, and died in 1980 at the age of 94.\n",
    "\n",
    "<br >\n",
    "\n",
    "### __Key Takeaway__ \n",
    "\n",
    "As engineers and business professionals, we are trained to as ourselves, what could we do to improve on an 80 percent average. As it is often the case, it’s easy to forget that these data points represent real people. Each time our model was wrong we should be glad -- in such misclasifications we will likely find incredible stories of human nature and courage triumphing over extremely difficult odds. \n",
    "\n",
    "__It is important to never lose sight of the human element when analyzing data that deals with people.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec7'></a>\n",
    "___\n",
    "## Appendix II: Resources and references to material we won't cover in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Gradient Boosting:** http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "> * **Jupyter Notebook (tutorial):** https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "\n",
    "> * **K-Nearest Neighbors (KNN):** https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26\n",
    "\n",
    "> * **Logistic Regression:** https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4\n",
    "\n",
    "> * **Naive Bayes:** http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "> * **Perceptron:** http://aass.oru.se/~lilien/ml/seminars/2007_02_01b-Janecek-Perceptron.pdf\n",
    "\n",
    "> * **Random Forest:** https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n",
    "\n",
    "> * **Support Vector Machines (SVM):** https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "\n",
    "\n",
    "<br>\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
