{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real World: AI, Machine Learning & Data Science \n",
    "\n",
    "---\n",
    "\n",
    "# Note: on data collection\n",
    "\n",
    "- Collect all the data you can! (storage is cheap)\n",
    "\n",
    "---\n",
    "\n",
    "# Business value from real example\n",
    "\n",
    "- Make correct business decisions\n",
    "- Ask the right questions (fair help from consultants, startups or data analytics companies)\n",
    "\n",
    "# Demystify\n",
    "\n",
    "This is a real world example of how you'd solve a Machine Learning prediciton problem.\n",
    "\n",
    "**Use Cases:**\n",
    "- Discover churn risk of customers\n",
    "- Predict optimal price levels (investments / retail)\n",
    "- Predict future revenues\n",
    "- Build recommendation systems\n",
    "- Customer value scoring\n",
    "- Fraud detection\n",
    "- Customer insights (characteristics)\n",
    "- Predict sentiment of text\n",
    "- Object detecton in images\n",
    "- etc etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Python?\n",
    "\n",
    "Python is general purpose and can do Software development, Web development, AI. Python has experienced incredible growth over the last couple of years, and many of the state of the art Machine Learning libraries being developed today have support for Python.\n",
    "\n",
    "<img src='https://zgab33vy595fw5zq-zippykid.netdna-ssl.com/wp-content/uploads/2017/09/growth_major_languages-1-1400x1200.png' width=400px></img>\n",
    "\n",
    "Source: https://stackoverflow.blog/2017/09/06/incredible-growth-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything is free!\n",
    "\n",
    "The best software today is open source and it's also enterprise-ready. Anyone can download and use them for free (even for business purposes).\n",
    "\n",
    "**Examples of great, free AI libraries:**\n",
    "* Anaconda\n",
    "* Google's TensorFlow\n",
    "* Scikit-learn\n",
    "* Pandas\n",
    "* Keras\n",
    "* Matplotlib\n",
    "* SQL\n",
    "* Spark\n",
    "* Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-of-the-Art algorithms\n",
    "\n",
    "No matter what algorithm you want to use (Linear Regression, Random Forests, Neural Networks, or Deep Learning), **all of the latest methods are implemented optimized for Python**.\n",
    "\n",
    "## Big Data\n",
    "\n",
    "Python code can run on any computer. Therefore, you can scale your computations and utilize for example cloud resources to run big data jobs.\n",
    "\n",
    "**Great tools for Big Data:**\n",
    "- Spark\n",
    "- Databricks\n",
    "- Hadoop / MapReduce\n",
    "- Kafka\n",
    "- Amazon EC2\n",
    "- Amazon S3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Real world example of AI: Titanic Analysis\n",
    "\n",
    "Titanic notebook is open source. All of our material is online. Anyone can develop the most sophisticated AI solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difficult part is never to implement the algorithm\n",
    "\n",
    "The hard part of a machine learning problem is to get data into the right format so you can solve the problem. We'll illustrate this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![data-x](http://oi64.tinypic.com/o858n4.jpg)\n",
    "\n",
    "\n",
    "# __Titanic Survivor Analysis__\n",
    "\n",
    "\n",
    "**Sources:** \n",
    "* **Training + explanations**: https://www.kaggle.com/c/titanic\n",
    "\n",
    "___\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Understanding the connections between passanger information and survival rate\n",
    "\n",
    "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others.\n",
    "\n",
    "### **Our task is to train a machine learning model that analyzes the trend and the information in the data in order to predict if the passengers survived or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB # Gaussian Naive Bays\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from plot_distribution import plot_distribution\n",
    "plt.rcParams['figure.figsize'] = (9, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec3'></a>\n",
    "___\n",
    "## Part 2: Exploring the Data\n",
    "**Data descriptions**\n",
    "\n",
    "<img src=\"data/Titanic_Variable.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the data\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General data statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(13,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced data set?\n",
    "df['Survived'].map({0:'Deceased',1:'Survived'}).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "> #### __Brief Remarks Regarding the Data__\n",
    "\n",
    "> * `PassengerId` is a random number (incrementing index) and thus does not contain any valuable information. \n",
    "\n",
    "> * `Survived, Passenger Class, Age, Siblings Spouses, Parents Children` and `Fare` are numerical values (no need to transform them) -- but, we might want to group them (i.e. create categorical variables). \n",
    "\n",
    "> * `Sex, Embarked` are categorical features that we need to map to integer values. `Name, Ticket` and `Cabin` might also contain valuable information.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unnecessary data\n",
    "__Note:__ It is important to remove variables that convey information already captured by some other variable. Doing so removes the correlation, while also diminishing potential overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns 'Ticket', 'Cabin', 'Fare' need to do it \n",
    "# for both test and training\n",
    "\n",
    "df = df.drop(['PassengerId','Ticket', 'Cabin','Fare'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec4'></a>\n",
    "____\n",
    "## Part 3: Transforming the data\n",
    "\n",
    "### 3.1 _The Title of the person can be used to predict survival_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List example titles in Name column\n",
    "df.Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column called Title\n",
    "\n",
    "df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that our titles makes sense (by comparing to sex)\n",
    "\n",
    "pd.crosstab(df['Title'], df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].\\\n",
    "              replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr',\\\n",
    "             'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "df['Title'] = df['Title'].replace('Mlle', 'Miss') #Mademoiselle\n",
    "df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "df['Title'] = df['Title'].replace('Mme', 'Mrs') #Madame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have more logical (contemporary) titles, and fewer groups\n",
    "\n",
    "df[['Title', 'Survived']].groupby(['Title']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can plot the survival chance for each title\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"Title\", data=df, order=[1,0])\n",
    "plt.xticks(range(2),['Survived','Deceased']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title dummy mapping: Map titles to binary dummy columns\n",
    "\n",
    "binary_encoded = pd.get_dummies(df.Title)\n",
    "df[binary_encoded.columns] = binary_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unique variables for analysis (Title is generally bound to Name, so it's also dropped)\n",
    "df = df.drop(['Name', 'Title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Gender column to binary (male = 0, female = 1) categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert categorical variable to numeric\n",
    "\n",
    "df['Sex'] = df['Sex']. \\\n",
    "    map( {'female': 1, 'male': 0} ).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle missing values for age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age = df.Age.fillna(df.Age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split age into bands and look at survival rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age bands\n",
    "df['AgeBand'] = pd.cut(df['Age'], 5)\n",
    "df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False)\\\n",
    "                    .mean().sort_values(by='AgeBand', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suvival probability against age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of Age of passangers who survived \n",
    "# or did not survive\n",
    "\n",
    "plot_distribution( df , var = 'Age' , target = 'Survived' ,\\\n",
    "                  row = 'Sex' )\n",
    "\n",
    "# Recall: {'male': 0, 'female': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Age column to\n",
    "# map Age ranges (AgeBands) to integer values of categorical type \n",
    "\n",
    "df.loc[ df['Age'] <= 16, 'Age'] = 0\n",
    "df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\n",
    "df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2\n",
    "df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3\n",
    "df.loc[ df['Age'] > 64, 'Age']=4\n",
    "df = df.drop(['AgeBand'], axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "# Note we could just run \n",
    "# df['Age'] = pd.cut(df['Age'], 5,labels=[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travel Party Size\n",
    "\n",
    "How did the number of people the person traveled with impact the chance of survival?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SibSp = Number of Sibling / Spouses\n",
    "# Parch = Parents / Children\n",
    "\n",
    "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "\n",
    "# Survival chance against FamilySize\n",
    "df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True) \\\n",
    "                                .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it, 1 is survived\n",
    "\n",
    "sns.countplot(x='Survived', hue=\"FamilySize\", data=df, order=[1,0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variable if the person was alone or not\n",
    "\n",
    "df['IsAlone'] = 0\n",
    "df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will only use the binary IsAlone feature for further analysis\n",
    "\n",
    "df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also create new features based on intuitive combinations\n",
    "# Here is an example when we say that the age times socioclass is a determinant factor\n",
    "\n",
    "df['Age*Class'] = df.Age.values * df.Pclass.values\n",
    "\n",
    "df.loc[:, ['Age*Class', 'Age', 'Pclass']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port the person embarked from\n",
    "Let's see how that influences chance of survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"data/images/titanic_voyage_map.png\">\n",
    ">___\n",
    "\n",
    "> #### __Interesting Fact:__ \n",
    "\n",
    "> Third Class passengers were the first to board, with First and Second Class passengers following up to an hour before departure. \n",
    "\n",
    "> Third Class passengers were inspected for ailments and physical impairments that might lead to their being refused entry to the United States, while First Class passengers were personally greeted by Captain Smith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN 'Embarked' Values in the dfs\n",
    "freq_port = df['Embarked'].dropna().mode()[0]\n",
    "df['Embarked'] = df['Embarked'].fillna(freq_port)\n",
    "    \n",
    "df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True) \\\n",
    "                    .mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical dummy variables for Embarked values\n",
    "\n",
    "binary_encoded = pd.get_dummies(df.Embarked)\n",
    "df[binary_encoded.columns] = binary_encoded\n",
    "df.drop('Embarked', axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finished -- Preprocessing Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features are approximately on the same scale\n",
    "# no need for feature engineering / normalization\n",
    "\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: View the correlation between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncorrelated features are generally more powerful predictors\n",
    "\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(df.corr().round(2)\\\n",
    "            ,linewidths=0.1,vmax=1.0, square=True, cmap=colormap, \\\n",
    "            linecolor='white', annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec5'></a>\n",
    "___\n",
    "### Machine Learning, Prediction and Artifical Intelligence\n",
    "Now we will use Machine Learning algorithms in order to predict if the person survived. \n",
    "\n",
    "**We will choose the best model from:**\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN) \n",
    "3. Support Vector Machines (SVM)\n",
    "4. Perceptron\n",
    "5. XGBoost\n",
    "6. Random Forest\n",
    "7. Neural Network (Deep Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Survived\", axis=1) # Training & Validation data\n",
    "Y = df[\"Survived\"]              # Response / Target Variable\n",
    "\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training set so that we validate on 20% of the data\n",
    "# Note that our algorithms will never have seen the validation \n",
    "\n",
    "np.random.seed(1337) # set random seed for reproducibility\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = \\\n",
    "                train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "print('Training Samples:', X_train.shape, Y_train.shape)\n",
    "print('Validation Samples:', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "> ## General ML workflow\n",
    "> 1. Create Model Object\n",
    "> 2. Train the Model\n",
    "> 3. Predict on _unseen_ data\n",
    "> 4. Evaluate accuracy.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Prediciton Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()           # create\n",
    "logreg.fit(X_train, Y_train)            # train\n",
    "acc_log_2 = logreg.score(X_val, Y_val)  # predict & evaluate\n",
    "\n",
    "print('Logistic Regression accuracy:',\\\n",
    "      str(round(acc_log_2*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)                  # instantiate\n",
    "knn.fit(X_train, Y_train)                                    # fit\n",
    "acc_knn = knn.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('K-Nearest Neighbors labeling accuracy:', str(round(acc_knn*100,2)),'%')                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines Classifier (non-linear kernel)\n",
    "svc = SVC()                                                  # instantiate\n",
    "svc.fit(X_train, Y_train)                                    # fit\n",
    "acc_svc = svc.score(X_val, Y_val)                            # predict + evaluate\n",
    "\n",
    "print('Support Vector Machines labeling accuracy:', str(round(acc_svc*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron = Perceptron()                                    # instantiate \n",
    "perceptron.fit(X_train, Y_train)                             # fit\n",
    "acc_perceptron = perceptron.score(X_val, Y_val)              # predict + evalaute\n",
    "\n",
    "print('Perceptron labeling accuracy:', str(round(acc_perceptron*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost, same API as scikit-learn\n",
    "gradboost = xgb.XGBClassifier(n_estimators=1000)             # instantiate\n",
    "gradboost.fit(X_train, Y_train)                              # fit\n",
    "acc_xgboost = gradboost.score(X_val, Y_val)                  # predict + evalute\n",
    "\n",
    "print('XGBoost labeling accuracy:', str(round(acc_xgboost*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=500)   # instantiate\n",
    "random_forest.fit(X_train, Y_train)                         # fit\n",
    "acc_rf = random_forest.score(X_val, Y_val)                  # predict + evaluate\n",
    "\n",
    "print('Random Forest labeling accuracy:', str(round(acc_rf*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Neural Networks (Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Dense(units=300, activation='relu', input_shape=(13,) ))\n",
    "model.add( Dense(units=100, activation='relu'))\n",
    "model.add( Dense(units=50, activation='relu'))\n",
    "model.add( Dense(units=1, activation='sigmoid') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs = 100, batch_size= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model Accuracy on test set\n",
    "print('Neural Network accuracy:',str(round(model.evaluate(X_val, Y_val, batch_size=50,verbose=False)[1]*100,2)),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance scores in the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at importnace of features for random forest\n",
    "\n",
    "def plot_model_var_imp( model , X , y ):\n",
    "    imp = pd.DataFrame( \n",
    "        model.feature_importances_  , \n",
    "        columns = [ 'Importance' ] , \n",
    "        index = X.columns \n",
    "    )\n",
    "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n",
    "    imp[ : 10 ].plot( kind = 'barh' )\n",
    "    print ('Training accuracy Random Forest:',model.score( X , y ))\n",
    "\n",
    "plot_model_var_imp(random_forest, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec6'></a>\n",
    "___\n",
    "\n",
    "## Appendix I:\n",
    "#### Why are our models maxing out at around 80%?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __John Jacob Astor__\n",
    "\n",
    "<img src= \"data/images/john-jacob-astor.jpg\"> \n",
    "\n",
    "John Jacob Astor perished in the disaster even though our model predicted he would survive. Astor was the wealthiest person on the Titanic -- his ticket fare was valued at over 35,000 USD in 2016 -- it seems likely that he would have been among of the approximatelly 35 percent of men in first class to survive. However, this was not the case: although his pregnant wife survived, John Jacob Astor’s body was recovered a week later, along with a gold watch, a diamond ring with three stones, and no less than 92,481 USD (2016 value) in cash.\n",
    "\n",
    "<br >\n",
    "\n",
    "\n",
    "#### __Olaus Jorgensen Abelseth__\n",
    "\n",
    "<img src= \"data/images/olaus-jorgensen-abelseth.jpg\">\n",
    "\n",
    "Avelseth was a 25-year-old Norwegian sailor, a man in 3rd class, and not expected to survive by classifier. However, once the ship sank, he survived by swimming for 20 minutes in the frigid North Atlantic water before joining other survivors on a waterlogged collapsible boat.\n",
    "\n",
    "Abelseth got married three years later, settled down as a farmer in North Dakota, had 4 kids, and died in 1980 at the age of 94.\n",
    "\n",
    "<br >\n",
    "\n",
    "### __Key Takeaway__ \n",
    "\n",
    "As engineers and business professionals, we are trained to as ourselves, what could we do to improve on an 80 percent average. As it is often the case, it’s easy to forget that these data points represent real people. Each time our model was wrong we should be glad -- in such misclasifications we will likely find incredible stories of human nature and courage triumphing over extremely difficult odds. \n",
    "\n",
    "__It is important to never lose sight of the human element when analyzing data that deals with people.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec7'></a>\n",
    "___\n",
    "## Appendix II: Resources and references to material we won't cover in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Gradient Boosting:** http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "> * **Jupyter Notebook (tutorial):** https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook\n",
    "\n",
    "> * **K-Nearest Neighbors (KNN):** https://towardsdatascience.com/introduction-to-k-nearest-neighbors-3b534bb11d26\n",
    "\n",
    "> * **Logistic Regression:** https://towardsdatascience.com/5-reasons-logistic-regression-should-be-the-first-thing-you-learn-when-become-a-data-scientist-fcaae46605c4\n",
    "\n",
    "> * **Naive Bayes:** http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "> * **Perceptron:** http://aass.oru.se/~lilien/ml/seminars/2007_02_01b-Janecek-Perceptron.pdf\n",
    "\n",
    "> * **Random Forest:** https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n",
    "\n",
    "> * **Support Vector Machines (SVM):** https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
    "\n",
    "\n",
    "<br>\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://i67.tinypic.com/2jcbwcw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
